{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a5259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.10.0+cu130\n",
      "Device:  cuda\n",
      "Train images:  Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: .data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n",
      "Image:  tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
      "          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
      "           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
      "           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
      "           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
      "          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
      "          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
      "           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
      "           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
      "           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
      "           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
      "           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
      "           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
      "           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n",
      "Label:  5\n",
      "Label one hot:  tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "\n",
      "Loading MNIST  train  Dataset...\n",
      "\tTotal Len.:  60000 \n",
      " --------------------------------------------------\n",
      "\n",
      "Loading MNIST  test  Dataset...\n",
      "\tTotal Len.:  10000 \n",
      " --------------------------------------------------\n",
      "Num workers 0\n",
      "Net(\n",
      "  (linear1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (BatchNorm1d1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (BatchNorm1d2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.3, inplace=False)\n",
      "  (linear3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (BatchNorm1d3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (drop3): Dropout(p=0.4, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Params:  1466122\n",
      "\n",
      "---- Start Training ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 600/600 [00:17<00:00, 34.46batch/s]\n",
      "Test 0: 100%|██████████| 100/100 [00:02<00:00, 36.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.450284 - Test Loss: 0.138924 - Train Accuracy: 89.04% - Test Accuracy: 96.06%\n",
      "EarlyStopping: best=96.06% (epoch 1) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 600/600 [00:17<00:00, 35.17batch/s]\n",
      "Test 1: 100%|██████████| 100/100 [00:02<00:00, 35.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.151790 - Test Loss: 0.088651 - Train Accuracy: 95.88% - Test Accuracy: 97.15%\n",
      "EarlyStopping: best=97.15% (epoch 2) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 600/600 [00:18<00:00, 33.30batch/s]\n",
      "Test 2: 100%|██████████| 100/100 [00:02<00:00, 34.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.108013 - Test Loss: 0.074144 - Train Accuracy: 96.89% - Test Accuracy: 97.74%\n",
      "EarlyStopping: best=97.74% (epoch 3) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 600/600 [00:17<00:00, 33.50batch/s]\n",
      "Test 3: 100%|██████████| 100/100 [00:02<00:00, 34.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.088504 - Test Loss: 0.073512 - Train Accuracy: 97.28% - Test Accuracy: 97.79%\n",
      "EarlyStopping: best=97.79% (epoch 4) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 600/600 [00:18<00:00, 33.03batch/s]\n",
      "Test 4: 100%|██████████| 100/100 [00:02<00:00, 34.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.083253 - Test Loss: 0.075646 - Train Accuracy: 97.36% - Test Accuracy: 97.54%\n",
      "EarlyStopping: best=97.79% (epoch 4) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 600/600 [00:17<00:00, 35.15batch/s]\n",
      "Test 5: 100%|██████████| 100/100 [00:02<00:00, 36.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.076739 - Test Loss: 0.070216 - Train Accuracy: 97.61% - Test Accuracy: 97.82%\n",
      "EarlyStopping: best=97.82% (epoch 6) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 600/600 [00:17<00:00, 35.03batch/s]\n",
      "Test 6: 100%|██████████| 100/100 [00:02<00:00, 36.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.073606 - Test Loss: 0.073912 - Train Accuracy: 97.64% - Test Accuracy: 97.73%\n",
      "EarlyStopping: best=97.82% (epoch 6) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 600/600 [00:17<00:00, 34.91batch/s]\n",
      "Test 7: 100%|██████████| 100/100 [00:02<00:00, 36.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.069009 - Test Loss: 0.070464 - Train Accuracy: 97.81% - Test Accuracy: 97.90%\n",
      "EarlyStopping: best=97.90% (epoch 8) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 600/600 [00:17<00:00, 34.91batch/s]\n",
      "Test 8: 100%|██████████| 100/100 [00:02<00:00, 36.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.064802 - Test Loss: 0.077953 - Train Accuracy: 97.94% - Test Accuracy: 97.69%\n",
      "EarlyStopping: best=97.90% (epoch 8) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 600/600 [00:17<00:00, 34.60batch/s]\n",
      "Test 9: 100%|██████████| 100/100 [00:02<00:00, 36.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.063226 - Test Loss: 0.066058 - Train Accuracy: 98.01% - Test Accuracy: 97.84%\n",
      "EarlyStopping: best=97.90% (epoch 8) patience_counter=2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 600/600 [00:17<00:00, 34.92batch/s]\n",
      "Test 10: 100%|██████████| 100/100 [00:02<00:00, 36.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.060760 - Test Loss: 0.067462 - Train Accuracy: 98.05% - Test Accuracy: 97.86%\n",
      "EarlyStopping: best=97.90% (epoch 8) patience_counter=3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 600/600 [00:16<00:00, 36.40batch/s]\n",
      "Test 11: 100%|██████████| 100/100 [00:02<00:00, 39.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.060158 - Test Loss: 0.073428 - Train Accuracy: 98.05% - Test Accuracy: 97.65%\n",
      "EarlyStopping: best=97.90% (epoch 8) patience_counter=4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 600/600 [00:16<00:00, 35.80batch/s]\n",
      "Test 12: 100%|██████████| 100/100 [00:02<00:00, 36.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.056347 - Test Loss: 0.061541 - Train Accuracy: 98.22% - Test Accuracy: 98.11%\n",
      "EarlyStopping: best=98.11% (epoch 13) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 600/600 [00:17<00:00, 34.21batch/s]\n",
      "Test 13: 100%|██████████| 100/100 [00:02<00:00, 38.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.052013 - Test Loss: 0.068460 - Train Accuracy: 98.31% - Test Accuracy: 98.02%\n",
      "EarlyStopping: best=98.11% (epoch 13) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 600/600 [00:16<00:00, 36.69batch/s]\n",
      "Test 14: 100%|██████████| 100/100 [00:02<00:00, 37.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.050237 - Test Loss: 0.070712 - Train Accuracy: 98.42% - Test Accuracy: 98.07%\n",
      "EarlyStopping: best=98.11% (epoch 13) patience_counter=2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 600/600 [00:16<00:00, 36.36batch/s]\n",
      "Test 15: 100%|██████████| 100/100 [00:02<00:00, 38.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Loss: 0.048071 - Test Loss: 0.074332 - Train Accuracy: 98.48% - Test Accuracy: 98.11%\n",
      "EarlyStopping: best=98.11% (epoch 13) patience_counter=3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 600/600 [00:17<00:00, 34.82batch/s]\n",
      "Test 16: 100%|██████████| 100/100 [00:02<00:00, 35.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Loss: 0.048665 - Test Loss: 0.058903 - Train Accuracy: 98.44% - Test Accuracy: 98.22%\n",
      "EarlyStopping: best=98.22% (epoch 17) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 600/600 [00:17<00:00, 34.48batch/s]\n",
      "Test 17: 100%|██████████| 100/100 [00:02<00:00, 35.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Loss: 0.043084 - Test Loss: 0.070680 - Train Accuracy: 98.58% - Test Accuracy: 98.03%\n",
      "EarlyStopping: best=98.22% (epoch 17) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 600/600 [00:17<00:00, 34.14batch/s]\n",
      "Test 18: 100%|██████████| 100/100 [00:02<00:00, 35.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Train Loss: 0.039197 - Test Loss: 0.057949 - Train Accuracy: 98.76% - Test Accuracy: 98.30%\n",
      "EarlyStopping: best=98.30% (epoch 19) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 600/600 [00:17<00:00, 33.52batch/s]\n",
      "Test 19: 100%|██████████| 100/100 [00:02<00:00, 35.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Train Loss: 0.039432 - Test Loss: 0.059559 - Train Accuracy: 98.70% - Test Accuracy: 98.26%\n",
      "EarlyStopping: best=98.30% (epoch 19) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 600/600 [00:17<00:00, 34.06batch/s]\n",
      "Test 20: 100%|██████████| 100/100 [00:02<00:00, 36.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Train Loss: 0.035992 - Test Loss: 0.067082 - Train Accuracy: 98.84% - Test Accuracy: 98.10%\n",
      "EarlyStopping: best=98.30% (epoch 19) patience_counter=2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 600/600 [00:17<00:00, 35.02batch/s]\n",
      "Test 21: 100%|██████████| 100/100 [00:02<00:00, 34.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Train Loss: 0.035281 - Test Loss: 0.056323 - Train Accuracy: 98.89% - Test Accuracy: 98.49%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 600/600 [00:16<00:00, 35.56batch/s]\n",
      "Test 22: 100%|██████████| 100/100 [00:02<00:00, 38.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Train Loss: 0.031300 - Test Loss: 0.062212 - Train Accuracy: 98.98% - Test Accuracy: 98.28%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 600/600 [00:16<00:00, 35.87batch/s]\n",
      "Test 23: 100%|██████████| 100/100 [00:02<00:00, 38.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Train Loss: 0.030328 - Test Loss: 0.062342 - Train Accuracy: 99.02% - Test Accuracy: 98.39%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 600/600 [00:16<00:00, 36.86batch/s]\n",
      "Test 24: 100%|██████████| 100/100 [00:02<00:00, 38.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Train Loss: 0.026698 - Test Loss: 0.059466 - Train Accuracy: 99.09% - Test Accuracy: 98.42%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 600/600 [00:16<00:00, 36.99batch/s]\n",
      "Test 25: 100%|██████████| 100/100 [00:02<00:00, 38.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Train Loss: 0.025616 - Test Loss: 0.059966 - Train Accuracy: 99.18% - Test Accuracy: 98.49%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 600/600 [00:16<00:00, 36.19batch/s]\n",
      "Test 26: 100%|██████████| 100/100 [00:02<00:00, 38.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Train Loss: 0.023038 - Test Loss: 0.059906 - Train Accuracy: 99.28% - Test Accuracy: 98.42%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 600/600 [00:16<00:00, 36.17batch/s]\n",
      "Test 27: 100%|██████████| 100/100 [00:02<00:00, 36.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Train Loss: 0.021383 - Test Loss: 0.063534 - Train Accuracy: 99.29% - Test Accuracy: 98.48%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 600/600 [00:17<00:00, 34.93batch/s]\n",
      "Test 28: 100%|██████████| 100/100 [00:02<00:00, 36.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Train Loss: 0.021595 - Test Loss: 0.061707 - Train Accuracy: 99.24% - Test Accuracy: 98.35%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 600/600 [00:17<00:00, 34.81batch/s]\n",
      "Test 29: 100%|██████████| 100/100 [00:02<00:00, 36.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Train Loss: 0.019651 - Test Loss: 0.063161 - Train Accuracy: 99.36% - Test Accuracy: 98.38%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 600/600 [00:18<00:00, 32.52batch/s]\n",
      "Test 30: 100%|██████████| 100/100 [00:02<00:00, 34.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31] Train Loss: 0.019187 - Test Loss: 0.063622 - Train Accuracy: 99.40% - Test Accuracy: 98.42%\n",
      "EarlyStopping: best=98.49% (epoch 22) patience_counter=9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 600/600 [00:17<00:00, 34.25batch/s]\n",
      "Test 31: 100%|██████████| 100/100 [00:02<00:00, 34.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32] Train Loss: 0.016898 - Test Loss: 0.059427 - Train Accuracy: 99.49% - Test Accuracy: 98.58%\n",
      "EarlyStopping: best=98.58% (epoch 32) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 600/600 [00:17<00:00, 34.09batch/s]\n",
      "Test 32: 100%|██████████| 100/100 [00:02<00:00, 36.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33] Train Loss: 0.015917 - Test Loss: 0.058277 - Train Accuracy: 99.47% - Test Accuracy: 98.60%\n",
      "EarlyStopping: best=98.58% (epoch 32) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 600/600 [00:17<00:00, 35.15batch/s]\n",
      "Test 33: 100%|██████████| 100/100 [00:02<00:00, 36.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34] Train Loss: 0.016991 - Test Loss: 0.066414 - Train Accuracy: 99.47% - Test Accuracy: 98.44%\n",
      "EarlyStopping: best=98.58% (epoch 32) patience_counter=2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 600/600 [00:17<00:00, 35.22batch/s]\n",
      "Test 34: 100%|██████████| 100/100 [00:02<00:00, 36.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35] Train Loss: 0.014047 - Test Loss: 0.061252 - Train Accuracy: 99.51% - Test Accuracy: 98.52%\n",
      "EarlyStopping: best=98.58% (epoch 32) patience_counter=3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 600/600 [00:17<00:00, 35.29batch/s]\n",
      "Test 35: 100%|██████████| 100/100 [00:02<00:00, 36.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36] Train Loss: 0.013524 - Test Loss: 0.064143 - Train Accuracy: 99.57% - Test Accuracy: 98.56%\n",
      "EarlyStopping: best=98.58% (epoch 32) patience_counter=4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 600/600 [00:17<00:00, 35.00batch/s]\n",
      "Test 36: 100%|██████████| 100/100 [00:02<00:00, 36.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37] Train Loss: 0.012572 - Test Loss: 0.070742 - Train Accuracy: 99.58% - Test Accuracy: 98.48%\n",
      "EarlyStopping: best=98.58% (epoch 32) patience_counter=5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 600/600 [00:16<00:00, 37.06batch/s]\n",
      "Test 37: 100%|██████████| 100/100 [00:02<00:00, 39.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38] Train Loss: 0.012371 - Test Loss: 0.063690 - Train Accuracy: 99.60% - Test Accuracy: 98.52%\n",
      "EarlyStopping: best=98.58% (epoch 32) patience_counter=6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 600/600 [00:16<00:00, 37.16batch/s]\n",
      "Test 38: 100%|██████████| 100/100 [00:02<00:00, 39.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39] Train Loss: 0.012813 - Test Loss: 0.055516 - Train Accuracy: 99.58% - Test Accuracy: 98.83%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 600/600 [00:16<00:00, 36.24batch/s]\n",
      "Test 39: 100%|██████████| 100/100 [00:02<00:00, 37.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40] Train Loss: 0.010027 - Test Loss: 0.060633 - Train Accuracy: 99.66% - Test Accuracy: 98.63%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 600/600 [00:16<00:00, 36.77batch/s]\n",
      "Test 40: 100%|██████████| 100/100 [00:02<00:00, 37.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41] Train Loss: 0.009243 - Test Loss: 0.061651 - Train Accuracy: 99.71% - Test Accuracy: 98.60%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 600/600 [00:16<00:00, 37.04batch/s]\n",
      "Test 41: 100%|██████████| 100/100 [00:02<00:00, 39.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42] Train Loss: 0.008649 - Test Loss: 0.064129 - Train Accuracy: 99.70% - Test Accuracy: 98.70%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 600/600 [00:16<00:00, 36.91batch/s]\n",
      "Test 42: 100%|██████████| 100/100 [00:02<00:00, 38.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43] Train Loss: 0.007933 - Test Loss: 0.058747 - Train Accuracy: 99.75% - Test Accuracy: 98.75%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 600/600 [00:16<00:00, 37.04batch/s]\n",
      "Test 43: 100%|██████████| 100/100 [00:02<00:00, 38.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44] Train Loss: 0.008077 - Test Loss: 0.067778 - Train Accuracy: 99.72% - Test Accuracy: 98.70%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 600/600 [00:16<00:00, 37.03batch/s]\n",
      "Test 44: 100%|██████████| 100/100 [00:02<00:00, 38.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45] Train Loss: 0.007370 - Test Loss: 0.061528 - Train Accuracy: 99.73% - Test Accuracy: 98.66%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 600/600 [00:16<00:00, 37.08batch/s]\n",
      "Test 45: 100%|██████████| 100/100 [00:02<00:00, 39.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46] Train Loss: 0.006156 - Test Loss: 0.063015 - Train Accuracy: 99.79% - Test Accuracy: 98.64%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 600/600 [00:16<00:00, 37.16batch/s]\n",
      "Test 46: 100%|██████████| 100/100 [00:02<00:00, 39.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47] Train Loss: 0.006254 - Test Loss: 0.069423 - Train Accuracy: 99.80% - Test Accuracy: 98.68%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 600/600 [00:16<00:00, 37.18batch/s]\n",
      "Test 47: 100%|██████████| 100/100 [00:02<00:00, 39.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48] Train Loss: 0.005220 - Test Loss: 0.069681 - Train Accuracy: 99.84% - Test Accuracy: 98.60%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 600/600 [00:16<00:00, 37.25batch/s]\n",
      "Test 48: 100%|██████████| 100/100 [00:02<00:00, 39.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49] Train Loss: 0.005073 - Test Loss: 0.072014 - Train Accuracy: 99.85% - Test Accuracy: 98.64%\n",
      "EarlyStopping: best=98.83% (epoch 39) patience_counter=10/10\n",
      "Stopping early at epoch 49. Best was epoch 39 with acc 98.83%\n",
      "\n",
      "BEST TEST ACCURACY:  98.83  in epoch  38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test 48: 100%|██████████| 100/100 [00:02<00:00, 39.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final best acc:  98.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as  F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Torch version: \", torch. __version__)\n",
    "\n",
    "####################################################################\n",
    "# Set Device\n",
    "####################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Prepare Data\n",
    "####################################################################\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST('.data/', train=True, download=True, transform=transform)\n",
    "#? Considera usar transform=transforms.Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]) para centrar/escala antes del flatten.\n",
    "#train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST('.data/', train=False, download=True, transform=transform)\n",
    "#test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Train images: \", train_set)\n",
    "print(\"Image: \", train_set[0][0])\n",
    "print(\"Label: \", train_set[0][1])\n",
    "print(\"Label one hot: \", F.one_hot(torch.tensor(train_set[0][1]), num_classes=10))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Dataset Class\n",
    "####################################################################\n",
    "\n",
    "class MNIST_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, partition = \"train\"):\n",
    "\n",
    "        print(\"\\nLoading MNIST \", partition, \" Dataset...\")\n",
    "        self.data = data\n",
    "        self.partition = partition\n",
    "        print(\"\\tTotal Len.: \", len(self.data), \"\\n\", 50*\"-\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def from_pil_to_tensor(self, image):\n",
    "        return torchvision.transforms.ToTensor()(image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Image\n",
    "        image_tensor = self.data[idx][0].view(-1)\n",
    "        # PIL Image to torch tensor\n",
    "        #! image_tensor = self.from_pil_to_tensor(image)\n",
    "        # care! net expect a 784 size vector and our dataset\n",
    "        # provide 1x28x28 (channels, height, width) -> Reshape!\n",
    "        #! image_tensor = image_tensor.view(-1)\n",
    "#? Tambien puedes normalizar aqui (image_tensor = (image_tensor - mean) / std) si no usas transforms.\n",
    "\n",
    "        # Label\n",
    "        label = torch.tensor(self.data[idx][1])\n",
    "        # label = F.one_hot(label, num_classes=10).float()\n",
    "        label = torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "#? Alternativa: devuelve label como entero y usa CrossEntropyLoss(label_smoothing=0.1) para regularizar sin one-hot.\n",
    "\n",
    "        return {\"img\": image_tensor, \"label\": label}\n",
    "\n",
    "train_dataset = MNIST_dataset(train_set, partition=\"train\")\n",
    "test_dataset = MNIST_dataset(test_set, partition=\"test\")\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# DataLoader Class\n",
    "####################################################################\n",
    "\n",
    "batch_size = 100\n",
    "num_workers = 0\n",
    "print(\"Num workers\", num_workers)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=num_workers)\n",
    "#? Para GPU ayuda pin_memory=True y persistent_workers=True cuando num_workers>0.\n",
    "\n",
    "####################################################################\n",
    "# Early stopping Class\n",
    "####################################################################\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0, mode=\"max\"):\n",
    "        \"\"\"\n",
    "        patience: nº de epochs sin mejora para parar\n",
    "        min_delta: mejora mínima para considerar 'mejora real'\n",
    "        mode: \"max\" si monitorizas accuracy, \"min\" si monitorizas loss\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.best_state_dict = None\n",
    "        self.best_epoch = -1\n",
    "\n",
    "    def step(self, score, model, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            self.best_epoch = epoch\n",
    "            return False  # no parar\n",
    "\n",
    "        improved = (score > self.best_score + self.min_delta) if self.mode == \"max\" \\\n",
    "                   else (score < self.best_score - self.min_delta)\n",
    "\n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        return self.counter >= self.patience  # True => parar\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Neural Network Class\n",
    "####################################################################\n",
    "\n",
    "# Creating our Neural Network - Fully Connected\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        #* AÑADIDO CAPA BATCHNORM1D Y DROPOUT\n",
    "        self.linear1 = nn.Linear(784, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.BatchNorm1d1 = nn.BatchNorm1d(1024)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "        self.linear2 = nn.Linear(1024, 512)\n",
    "        self.BatchNorm1d2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.linear3 = nn.Linear(512, 256)\n",
    "        self.BatchNorm1d3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "#? BatchNorm1d tras cada Linear y Dropout(0.1-0.3) antes de la activacion suelen mejorar la generalizacion.\n",
    "#? Un MLP mas profundo pero mas estrecho (ej. 784->512->256->128->10) reduce parametros y overfitting sin usar CNN.\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.drop1(self.relu1(self.BatchNorm1d1(self.linear1(x))))\n",
    "        out = self.drop2(self.relu2(self.BatchNorm1d2(self.linear2(out))))\n",
    "        out = self.drop3(self.relu3(self.BatchNorm1d3(self.linear3(out))))\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiating the network and printing its architecture\n",
    "num_classes = 10\n",
    "net = Net(num_classes)\n",
    "print(net)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Params: \", count_parameters(net))\n",
    "\n",
    "####################################################################\n",
    "# Training settings\n",
    "####################################################################\n",
    "\n",
    "# Training hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, weight_decay=1e-6, momentum=0.9) # Original lr=0.01\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "epochs = 75 # Original = 25\n",
    "#? Prueba AdamW con weight_decay mas alto (p.ej. 1e-2) y un scheduler CosineAnnealingLR u OneCycleLR.\n",
    "\n",
    "#* SCHEDULER\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-3,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_dataloader)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Training\n",
    "####################################################################\n",
    "\n",
    "# Load model in GPU\n",
    "net.to(device)\n",
    "\n",
    "print(\"\\n---- Start Training ----\")\n",
    "best_accuracy = -1\n",
    "best_epoch = 0\n",
    "\n",
    "# early_stopper = EarlyStopping(patience=10, min_delta=0.02, mode=\"max\")\n",
    "# min_delta=0.05 significa +0.05% de accuracy como mejora mínima (ajústalo si quieres)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "    # TRAIN NETWORK\n",
    "    train_loss, train_correct = 0, 0\n",
    "    net.train()\n",
    "    with tqdm(iter(train_dataloader), desc=\"Epoch \" + str(epoch), unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "\n",
    "            # Returned values of Dataset Class\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "#? Puedes recortar gradientes con torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0) si ves inestabilidad.\n",
    "\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "            #* ACTUALIZADO PARA USAR SCHEDULER\n",
    "            scheduler.step()\n",
    "            #! # one hot -> labels\n",
    "            #! labels = torch.argmax(labels, dim=1)\n",
    "            #! pred = torch.argmax(outputs, dim=1)\n",
    "            #! train_correct += pred.eq(labels).sum().item()\n",
    "            \n",
    "            # sin one-hot\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            train_correct += pred.eq(labels).sum().item()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "\n",
    "    # TEST NETWORK\n",
    "    test_loss, test_correct = 0, 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      with tqdm(iter(test_dataloader), desc=\"Test \" + str(epoch), unit=\"batch\") as tepoch:\n",
    "          for batch in tepoch:\n",
    "\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = net(images)\n",
    "            test_loss += criterion(outputs, labels).item() * images.size(0)\n",
    "\n",
    "            #! # one hot -> labels\n",
    "            #! labels = torch.argmax(labels, dim=1)\n",
    "            #! pred = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # sin one-hot\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            ()\n",
    "\n",
    "            test_correct += pred.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = 100. * test_correct / len(test_dataloader.dataset)\n",
    "\n",
    "    print(\"[Epoch {}] Train Loss: {:.6f} - Test Loss: {:.6f} - Train Accuracy: {:.2f}% - Test Accuracy: {:.2f}%\".format(\n",
    "        epoch + 1, train_loss, test_loss, 100. * train_correct / len(train_dataloader.dataset), test_accuracy\n",
    "    ))\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_epoch = epoch\n",
    "\n",
    "        # Save best weights\n",
    "        torch.save(net.state_dict(), \"best_model.pt\")\n",
    "        \n",
    "    # should_stop = early_stopper.step(test_accuracy, net, epoch)\n",
    "    # print(f\"EarlyStopping: best={early_stopper.best_score:.2f}% (epoch {early_stopper.best_epoch+1}) \"\n",
    "    #     f\"patience_counter={early_stopper.counter}/{early_stopper.patience}\")\n",
    "\n",
    "    # if should_stop:\n",
    "    #     print(f\"Stopping early at epoch {epoch+1}. Best was epoch {early_stopper.best_epoch+1} \"\n",
    "    #         f\"with acc {early_stopper.best_score:.2f}%\")\n",
    "    #     break\n",
    "\n",
    "#? Agrega early stopping con paciencia (p.ej. 10 epocas) y ReduceLROnPlateau para bajar lr cuando el val loss se estanque.\n",
    "\n",
    "print(\"\\nBEST TEST ACCURACY: \", best_accuracy, \" in epoch \", best_epoch)\n",
    "\n",
    "# So far:\n",
    "# best acc:  98.24 (default)\n",
    "# best acc:  96.64 with lr: 0.001\n",
    "# best acc:  98.26 with 2 hidden layers\n",
    "# best acc:  98.64 with lr: 0.1\n",
    "# best acc:  98.02 with lr: 0.001 & 75 epochs\n",
    "\n",
    "####################################################################\n",
    "# Load best weights\n",
    "####################################################################\n",
    "\n",
    "# Load best weights\n",
    "net.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "test_loss, test_correct = 0, 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(iter(test_dataloader), desc=\"Test \" + str(epoch), unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = net(images)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "            #! # one hot -> labels\n",
    "            #! labels = torch.argmax(labels, dim=1)\n",
    "            #! pred = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            pred = outputs.argmax(dim=1)\n",
    "            ()\n",
    "\n",
    "            test_correct += pred.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = 100. * test_correct / len(test_dataloader.dataset)\n",
    "print(\"Final best acc: \", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
