{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.10.0+cu130\n",
      "Device:  cuda\n",
      "Train images:  Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: .data/\n",
      "    Split: Train\n",
      "Image:  <PIL.Image.Image image mode=L size=28x28 at 0x2A4841C8E30>\n",
      "Label:  5\n",
      "Label one hot:  tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "\n",
      "Loading MNIST  train  Dataset...\n",
      "\tTotal Len.:  60000 \n",
      " --------------------------------------------------\n",
      "\n",
      "Loading MNIST  test  Dataset...\n",
      "\tTotal Len.:  10000 \n",
      " --------------------------------------------------\n",
      "Num workers 0\n",
      "Net(\n",
      "  (linear1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (BatchNorm1d1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (BatchNorm1d2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.3, inplace=False)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (BatchNorm1d3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (drop3): Dropout(p=0.4, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Params:  569226\n",
      "\n",
      "---- Start Training ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 600/600 [00:06<00:00, 97.31batch/s]\n",
      "Test 0: 100%|██████████| 100/100 [00:00<00:00, 112.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.002914 - Test Loss: 0.001043 - Train Accuracy: 92.09% - Test Accuracy: 96.69%\n",
      "EarlyStopping: best=96.69% (epoch 1) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 600/600 [00:06<00:00, 98.25batch/s]\n",
      "Test 1: 100%|██████████| 100/100 [00:00<00:00, 112.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.001324 - Test Loss: 0.000782 - Train Accuracy: 96.09% - Test Accuracy: 97.40%\n",
      "EarlyStopping: best=97.40% (epoch 2) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 600/600 [00:06<00:00, 98.82batch/s]\n",
      "Test 2: 100%|██████████| 100/100 [00:00<00:00, 113.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.001032 - Test Loss: 0.000704 - Train Accuracy: 96.95% - Test Accuracy: 97.81%\n",
      "EarlyStopping: best=97.81% (epoch 3) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 600/600 [00:06<00:00, 98.47batch/s]\n",
      "Test 3: 100%|██████████| 100/100 [00:00<00:00, 112.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.000847 - Test Loss: 0.000650 - Train Accuracy: 97.46% - Test Accuracy: 98.05%\n",
      "EarlyStopping: best=98.05% (epoch 4) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 600/600 [00:06<00:00, 98.05batch/s]\n",
      "Test 4: 100%|██████████| 100/100 [00:00<00:00, 112.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.000732 - Test Loss: 0.000634 - Train Accuracy: 97.77% - Test Accuracy: 98.10%\n",
      "EarlyStopping: best=98.05% (epoch 4) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 600/600 [00:06<00:00, 98.21batch/s]\n",
      "Test 5: 100%|██████████| 100/100 [00:00<00:00, 114.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.000656 - Test Loss: 0.000577 - Train Accuracy: 98.00% - Test Accuracy: 98.16%\n",
      "EarlyStopping: best=98.16% (epoch 6) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 600/600 [00:06<00:00, 98.35batch/s]\n",
      "Test 6: 100%|██████████| 100/100 [00:00<00:00, 112.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.000562 - Test Loss: 0.000611 - Train Accuracy: 98.28% - Test Accuracy: 98.14%\n",
      "EarlyStopping: best=98.16% (epoch 6) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 600/600 [00:06<00:00, 98.32batch/s]\n",
      "Test 7: 100%|██████████| 100/100 [00:00<00:00, 112.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.000567 - Test Loss: 0.000544 - Train Accuracy: 98.31% - Test Accuracy: 98.36%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 600/600 [00:06<00:00, 98.29batch/s]\n",
      "Test 8: 100%|██████████| 100/100 [00:00<00:00, 111.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.000486 - Test Loss: 0.000573 - Train Accuracy: 98.45% - Test Accuracy: 98.24%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 600/600 [00:06<00:00, 93.98batch/s]\n",
      "Test 9: 100%|██████████| 100/100 [00:01<00:00, 88.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.000459 - Test Loss: 0.000544 - Train Accuracy: 98.55% - Test Accuracy: 98.38%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 600/600 [00:06<00:00, 92.69batch/s]\n",
      "Test 10: 100%|██████████| 100/100 [00:00<00:00, 102.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.000427 - Test Loss: 0.000591 - Train Accuracy: 98.64% - Test Accuracy: 98.30%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 600/600 [00:06<00:00, 92.76batch/s]\n",
      "Test 11: 100%|██████████| 100/100 [00:00<00:00, 112.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.000375 - Test Loss: 0.000601 - Train Accuracy: 98.78% - Test Accuracy: 98.21%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 600/600 [00:06<00:00, 98.55batch/s]\n",
      "Test 12: 100%|██████████| 100/100 [00:00<00:00, 112.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.000352 - Test Loss: 0.000598 - Train Accuracy: 98.93% - Test Accuracy: 98.36%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 600/600 [00:06<00:00, 98.41batch/s] \n",
      "Test 13: 100%|██████████| 100/100 [00:00<00:00, 112.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.000332 - Test Loss: 0.000564 - Train Accuracy: 98.92% - Test Accuracy: 98.39%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 600/600 [00:06<00:00, 98.36batch/s]\n",
      "Test 14: 100%|██████████| 100/100 [00:00<00:00, 111.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.000298 - Test Loss: 0.000584 - Train Accuracy: 99.03% - Test Accuracy: 98.37%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 600/600 [00:06<00:00, 94.68batch/s]\n",
      "Test 15: 100%|██████████| 100/100 [00:00<00:00, 100.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Loss: 0.000278 - Test Loss: 0.000615 - Train Accuracy: 99.15% - Test Accuracy: 98.32%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 600/600 [00:06<00:00, 90.12batch/s]\n",
      "Test 16: 100%|██████████| 100/100 [00:00<00:00, 102.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Loss: 0.000309 - Test Loss: 0.000571 - Train Accuracy: 99.06% - Test Accuracy: 98.37%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 600/600 [00:06<00:00, 92.52batch/s]\n",
      "Test 17: 100%|██████████| 100/100 [00:00<00:00, 104.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Loss: 0.000287 - Test Loss: 0.000620 - Train Accuracy: 99.03% - Test Accuracy: 98.36%\n",
      "EarlyStopping: best=98.36% (epoch 8) patience_counter=10/10\n",
      "Stopping early at epoch 18. Best was epoch 8 with acc 98.36%\n",
      "\n",
      "BEST TEST ACCURACY:  98.39  in epoch  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test 17: 100%|██████████| 100/100 [00:00<00:00, 107.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final best acc:  98.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as  F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Torch version: \", torch. __version__)\n",
    "\n",
    "####################################################################\n",
    "# Set Device\n",
    "####################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Prepare Data\n",
    "####################################################################\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST('.data/', train=True, download=True, transform=transform)\n",
    "#? Considera usar transform=transforms.Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]) para centrar/escala antes del flatten.\n",
    "#train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST('.data/', train=False, download=True, transform=transform)\n",
    "#test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Train images: \", train_set)\n",
    "print(\"Image: \", train_set[0][0])\n",
    "print(\"Label: \", train_set[0][1])\n",
    "print(\"Label one hot: \", F.one_hot(torch.tensor(train_set[0][1]), num_classes=10))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Dataset Class\n",
    "####################################################################\n",
    "\n",
    "class MNIST_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, partition = \"train\"):\n",
    "\n",
    "        print(\"\\nLoading MNIST \", partition, \" Dataset...\")\n",
    "        self.data = data\n",
    "        self.partition = partition\n",
    "        print(\"\\tTotal Len.: \", len(self.data), \"\\n\", 50*\"-\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def from_pil_to_tensor(self, image):\n",
    "        return torchvision.transforms.ToTensor()(image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Image\n",
    "        image_tensor = self.data[idx][0].view(-1)\n",
    "        # PIL Image to torch tensor\n",
    "        #! image_tensor = self.from_pil_to_tensor(image)\n",
    "        # care! net expect a 784 size vector and our dataset\n",
    "        # provide 1x28x28 (channels, height, width) -> Reshape!\n",
    "        #! image_tensor = image_tensor.view(-1)\n",
    "#? Tambien puedes normalizar aqui (image_tensor = (image_tensor - mean) / std) si no usas transforms.\n",
    "\n",
    "        # Label\n",
    "        label = torch.tensor(self.data[idx][1])\n",
    "        # label = F.one_hot(label, num_classes=10).float()\n",
    "        label = torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "#? Alternativa: devuelve label como entero y usa CrossEntropyLoss(label_smoothing=0.1) para regularizar sin one-hot.\n",
    "\n",
    "        return {\"img\": image_tensor, \"label\": label}\n",
    "\n",
    "train_dataset = MNIST_dataset(train_set, partition=\"train\")\n",
    "test_dataset = MNIST_dataset(test_set, partition=\"test\")\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# DataLoader Class\n",
    "####################################################################\n",
    "\n",
    "batch_size = 100\n",
    "num_workers = 0\n",
    "print(\"Num workers\", num_workers)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=num_workers)\n",
    "#? Para GPU ayuda pin_memory=True y persistent_workers=True cuando num_workers>0.\n",
    "\n",
    "####################################################################\n",
    "# Early stopping Class\n",
    "####################################################################\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0, mode=\"max\"):\n",
    "        \"\"\"\n",
    "        patience: nº de epochs sin mejora para parar\n",
    "        min_delta: mejora mínima para considerar 'mejora real'\n",
    "        mode: \"max\" si monitorizas accuracy, \"min\" si monitorizas loss\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.best_state_dict = None\n",
    "        self.best_epoch = -1\n",
    "\n",
    "    def step(self, score, model, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            self.best_epoch = epoch\n",
    "            return False  # no parar\n",
    "\n",
    "        improved = (score > self.best_score + self.min_delta) if self.mode == \"max\" \\\n",
    "                   else (score < self.best_score - self.min_delta)\n",
    "\n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        return self.counter >= self.patience  # True => parar\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Neural Network Class\n",
    "####################################################################\n",
    "\n",
    "# Creating our Neural Network - Fully Connected\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        #* AÑADIDO CAPA BATCHNORM1D Y DROPOUT\n",
    "        self.linear1 = nn.Linear(784, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.BatchNorm1d1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.BatchNorm1d2 = nn.BatchNorm1d(256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.linear3 = nn.Linear(256, 128)\n",
    "        self.BatchNorm1d3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.4)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "#? BatchNorm1d tras cada Linear y Dropout(0.1-0.3) antes de la activacion suelen mejorar la generalizacion.\n",
    "#? Un MLP mas profundo pero mas estrecho (ej. 784->512->256->128->10) reduce parametros y overfitting sin usar CNN.\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.drop1(self.relu1(self.BatchNorm1d1(self.linear1(x))))\n",
    "        out = self.drop2(self.relu2(self.BatchNorm1d2(self.linear2(out))))\n",
    "        out = self.drop3(self.relu3(self.BatchNorm1d3(self.linear3(out))))\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiating the network and printing its architecture\n",
    "num_classes = 10\n",
    "net = Net(num_classes)\n",
    "print(net)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Params: \", count_parameters(net))\n",
    "\n",
    "####################################################################\n",
    "# Training settings\n",
    "####################################################################\n",
    "\n",
    "# Training hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, weight_decay=1e-6, momentum=0.9) # Original lr=0.01\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "epochs = 75 # Original = 25\n",
    "#? Prueba AdamW con weight_decay mas alto (p.ej. 1e-2) y un scheduler CosineAnnealingLR u OneCycleLR.\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Training\n",
    "####################################################################\n",
    "\n",
    "# Load model in GPU\n",
    "net.to(device)\n",
    "\n",
    "print(\"\\n---- Start Training ----\")\n",
    "best_accuracy = -1\n",
    "best_epoch = 0\n",
    "\n",
    "early_stopper = EarlyStopping(patience=10, min_delta=0.1, mode=\"max\")\n",
    "# min_delta=0.05 significa +0.05% de accuracy como mejora mínima (ajústalo si quieres)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "    # TRAIN NETWORK\n",
    "    train_loss, train_correct = 0, 0\n",
    "    net.train()\n",
    "    with tqdm(iter(train_dataloader), desc=\"Epoch \" + str(epoch), unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "\n",
    "            # Returned values of Dataset Class\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "#? Puedes recortar gradientes con torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0) si ves inestabilidad.\n",
    "\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # one hot -> labels\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            train_correct += pred.eq(labels).sum().item()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "\n",
    "    # TEST NETWORK\n",
    "    test_loss, test_correct = 0, 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      with tqdm(iter(test_dataloader), desc=\"Test \" + str(epoch), unit=\"batch\") as tepoch:\n",
    "          for batch in tepoch:\n",
    "\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = net(images)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "            # one hot -> labels\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            test_correct += pred.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = 100. * test_correct / len(test_dataloader.dataset)\n",
    "\n",
    "    print(\"[Epoch {}] Train Loss: {:.6f} - Test Loss: {:.6f} - Train Accuracy: {:.2f}% - Test Accuracy: {:.2f}%\".format(\n",
    "        epoch + 1, train_loss, test_loss, 100. * train_correct / len(train_dataloader.dataset), test_accuracy\n",
    "    ))\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_epoch = epoch\n",
    "\n",
    "        # Save best weights\n",
    "        torch.save(net.state_dict(), \"best_model.pt\")\n",
    "        \n",
    "    should_stop = early_stopper.step(test_accuracy, net, epoch)\n",
    "    print(f\"EarlyStopping: best={early_stopper.best_score:.2f}% (epoch {early_stopper.best_epoch+1}) \"\n",
    "        f\"patience_counter={early_stopper.counter}/{early_stopper.patience}\")\n",
    "\n",
    "    if should_stop:\n",
    "        print(f\"Stopping early at epoch {epoch+1}. Best was epoch {early_stopper.best_epoch+1} \"\n",
    "            f\"with acc {early_stopper.best_score:.2f}%\")\n",
    "        break\n",
    "\n",
    "#? Agrega early stopping con paciencia (p.ej. 10 epocas) y ReduceLROnPlateau para bajar lr cuando el val loss se estanque.\n",
    "\n",
    "print(\"\\nBEST TEST ACCURACY: \", best_accuracy, \" in epoch \", best_epoch)\n",
    "\n",
    "# So far:\n",
    "# best acc:  98.24 (default)\n",
    "# best acc:  96.64 with lr: 0.001\n",
    "# best acc:  98.26 with 2 hidden layers\n",
    "# best acc:  98.64 with lr: 0.1\n",
    "# best acc:  98.02 with lr: 0.001 & 75 epochs\n",
    "\n",
    "####################################################################\n",
    "# Load best weights\n",
    "####################################################################\n",
    "\n",
    "# Load best weights\n",
    "net.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "test_loss, test_correct = 0, 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(iter(test_dataloader), desc=\"Test \" + str(epoch), unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = net(images)\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "            # one hot -> labels\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            test_correct += pred.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = 100. * test_correct / len(test_dataloader.dataset)\n",
    "print(\"Final best acc: \", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
